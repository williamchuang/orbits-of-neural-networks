%CUT AND PASTE THIS ENTIRE DOCUMENT INTO A BLANK DOCUMENT ON CloudSageMath. Then replace the content (title, name, and so on), with your stuff. If you have problems or errors, let me know! 

%\documentclass[12pt]{amsart}
\documentclass{article}

\usepackage{tikz-cd}

%Python Code Listing
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}

\usepackage{xcolor}
\definecolor{maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{halfgray}{gray}{0.55}
\definecolor{ipython_frame}{RGB}{207, 207, 207}
\definecolor{ipython_bg}{RGB}{247, 247, 247}
\definecolor{ipython_red}{RGB}{186, 33, 33}
\definecolor{ipython_green}{RGB}{0, 128, 0}
\definecolor{ipython_cyan}{RGB}{64, 128, 128}
\definecolor{ipython_purple}{RGB}{170, 34, 255}



\usepackage{listings}
\lstset{
    breaklines=true,
    %
    extendedchars=true,
    literate=
    {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
    {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
    {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
    {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
    {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
    {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
    {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
    { }{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
    {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
    {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
    {€}{{\EUR}}1 {£}{{\pounds}}1
}

%%
%% Python definition (c) 1998 Michael Weber
%% Additional definitions (2013) Alexis Dimitriadis
%% modified by me (should not have empty lines)
%%
\lstdefinelanguage{iPython}{
    morekeywords={access,and,break,class,continue,def,del,elif,else,except,exec,finally,for,from,global,if,import,in,is,lambda,not,or,pass,print,raise,return,try,while},%
    %
    % Built-ins
    morekeywords=[2]{abs,all,any,basestring,bin,bool,bytearray,callable,chr,classmethod,cmp,compile,complex,delattr,dict,dir,divmod,enumerate,eval,execfile,file,filter,float,format,frozenset,getattr,globals,hasattr,hash,help,hex,id,input,int,isinstance,issubclass,iter,len,list,locals,long,map,max,memoryview,min,next,object,oct,open,ord,pow,property,range,raw_input,reduce,reload,repr,reversed,round,set,setattr,slice,sorted,staticmethod,str,sum,super,tuple,type,unichr,unicode,vars,xrange,zip,apply,buffer,coerce,intern},%
    %
    sensitive=true,%
    morecomment=[l]\#,%
    morestring=[b]',%
    morestring=[b]",%
    %
    morestring=[s]{'''}{'''},% used for documentation text (mulitiline strings)
    morestring=[s]{"""}{"""},% added by Philipp Matthias Hahn
    %
    morestring=[s]{r'}{'},% `raw' strings
    morestring=[s]{r"}{"},%
    morestring=[s]{r'''}{'''},%
    morestring=[s]{r"""}{"""},%
    morestring=[s]{u'}{'},% unicode strings
    morestring=[s]{u"}{"},%
    morestring=[s]{u'''}{'''},%
    morestring=[s]{u"""}{"""},%
    %
    % {replace}{replacement}{lenght of replace}
    % *{-}{-}{1} will not replace in comments and so on
    literate=
    {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
    {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
    {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
    {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
    {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
    {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
    {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
    { }{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
    {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
    {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
    {€}{{\EUR}}1 {£}{{\pounds}}1
    %
    {^}{{{\color{ipython_purple}\^{}}}}1
    {=}{{{\color{ipython_purple}=}}}1
    %
    {+}{{{\color{ipython_purple}+}}}1
    {*}{{{\color{ipython_purple}$^\ast$}}}1
    {/}{{{\color{ipython_purple}/}}}1
    %
    {+=}{{{+=}}}1
    {-=}{{{-=}}}1
    {*=}{{{$^\ast$=}}}1
    {/=}{{{/=}}}1,
    literate=
    *{-}{{{\color{ipython_purple}-}}}1
     {?}{{{\color{ipython_purple}?}}}1,
    %
    identifierstyle=\color{black}\ttfamily,
    commentstyle=\color{ipython_cyan}\ttfamily,
    stringstyle=\color{ipython_red}\ttfamily,
    keepspaces=true,
    showspaces=false,
    showstringspaces=false,
    %
    rulecolor=\color{ipython_frame},
    frame=single,
    frameround={t}{t}{t}{t},
    framexleftmargin=6mm,
    numbers=left,
    numberstyle=\tiny\color{halfgray},
    %
    %
    backgroundcolor=\color{ipython_bg},
    %   extendedchars=true,
    basicstyle=\scriptsize,
    keywordstyle=\color{ipython_green}\ttfamily,
}

%%%%flowchart
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text width=25em,text centered, draw=black, fill=yellow!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text width=15em, text centered, draw=black, fill=orange!30]
%\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

\tikzstyle{decision} = [rectangle, rounded corners, draw, fill=blue!20, 
    text width=15em, text badly centered, node distance=2.2cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]
%\usepackage[english]{babel}
%\usepackage[utf8]{inputenc}
%\usepackage[margin=1in]{geometry}
%\usepackage[titletoc,title]{appendix}
\usepackage{array}
\usepackage{amsmath,amsfonts,amssymb,mathtools}
\usepackage{amsthm}
%These commands deal with theorem-like environments (i.e., italic)
\theoremstyle{plain}
%\newtheorem{theorem}{Theorem}
%\newtheorem{proposition}{Proposition}
%\newtheorem{corollary}{Corollary}
\newtheorem{property}{Property}
%\newtheorem{definition}{Definition}
%\newtheorem{proof}{Proof}
%\newtheorem{lemma}{Lemma}
%\newtheorem{claim}{Claim}
%\newtheorem{example}{Example}
\newtheorem{non-example}{Non-Example}

\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\Tr}{Tr}


\usepackage{hyperref}

\usepackage{graphicx}
\usepackage{tikz,lipsum,lmodern}
\usepackage[most]{tcolorbox}

%\thispagestyle{empty}

%%%%%%from math 729

\usepackage[]{algorithm2e}
\usepackage{graphicx,float}
\usepackage{tikz}
\usetikzlibrary{graphs,graphs.standard,quotes}
\usepackage{multicol}
\usepackage{capt-of}

\usepackage{amscd,amsthm,amsmath,amssymb,amsfonts,latexsym,
graphicx,enumerate,setspace,verbatim,tocloft,rotating}
%\usepackage{color}                    % For creating colored text and background
%\usepackage{hyperref}                 % For creating hyperlinks in cross references
% other possibly useful packages: textcomp,mathrsfs,amscd,epsfig,euscript,cancel

%%%%% Layout
% These numbers might depend on your printer. Check the margins and compare them to the Graduate Division's
% guidelines. If there's something off, try playing with the numbers...
%
% For chapters:
%     Must have a minimum of 1.5in margin on left and 1in on all other sides.  Where there are page numbers
%     (whether on top or bottom), must have one additional inch between the page number and the text, for a
%     total of 2in between the edge of the paper and the text.
% For frontmatter pages:
%     The same margin numbers generally work, except for the Title Page, so you will notice that we use
%     some numbers for \textheight and \footskip right here, and then change them below, right after
%     generating the Title Page.

\hoffset=.5in 
\oddsidemargin=0in   % = 1in because LaTeX adds 1in
\evensidemargin=0in  % = 1in because LaTeX adds 1in
\topmargin=0in       % = 1in because LaTeX adds 1in
\headheight=0in
\headsep=1in         % Distance from top of pagenum (for page numbers at top-right corner of page) to text
\footskip=1.2in      % Distance from bottom of text to the page number (for page number at bottom of page)
\textwidth=5.9in     % Should be 6in, but use 5.9in to be conservative
\textheight=8.0in    % Best for Title Page (will change after the Title Page)

\pagestyle{plain}

\doublespacing

%%%%% Style of theorems, definitions, examples, equations, etc.

\theoremstyle{plain} % Heading is bold, text italic.
%\newtheorem{theorem}{Theorem}[chapter]
%\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{conjecture}{Conjecture}[chapter]
\newtheorem*{utheorem}{Theorem}
\newtheorem*{ucorollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{ulemma}{Lemma}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{definition}  % Heading is bold, text is roman
%\newtheorem{definition}{Definition}[chapter]
%\newtheorem{example}{Example}[chapter]

\theoremstyle{remark}  % Heading is italic, text is roman
\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}
%\newtheorem{claim}{Claim}[chapter]

% Shortcuts (add your own...):
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\e}[1]{{\mathbb E}\left[ #1 \right]}


%%%%flowchart
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text width=25em,text centered, draw=black, fill=yellow!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text width=15em, text centered, draw=black, fill=orange!30]
%\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

\tikzstyle{decision} = [rectangle, rounded corners, draw, fill=blue!20, 
    text width=15em, text badly centered, node distance=2.2cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]
%\usepackage[english]{babel}
%\usepackage[utf8]{inputenc}
%\usepackage[margin=1in]{geometry}
%\usepackage[titletoc,title]{appendix}
\usepackage{array}
\usepackage{amsmath,amsfonts,amssymb,mathtools}
\usepackage{amsthm}
%These commands deal with theorem-like environments (i.e., italic)
\theoremstyle{plain}


%\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
%\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%%%%% Appendix style

\renewcommand\appendix[1]{
\chapter*{#1}
\addcontentsline{toc}{chapter}{#1}
}


\begin{document}

\title{The orbits of neural networks}

\author{William Chuang}

\maketitle

%\begin{tcolorbox}[enhanced,fit to height=10cm,
%  colback=green!25!black!10!white,colframe=green!75!black,title=Section 3.2.1 in 1706.03762,
%  drop fuzzy shadow,watermark color=white]
%\includegraphics[scale=0.4]{1.png}
%\end{tcolorbox}

\section{Introduction}
Inspired by \cite{yun2017global, blum1988training, judd1990neural}, consider the following question: let a training set and a trained neural network be given where the trained neural network already converged to a local minimum, meaning given a positive real number $\epsilon>0$, then each sample $X_i$ in the given training set, there is $N\in\mathbb{N}$ such that for the number of epoch greater than $N$, the model is in the epsilon ball around the local minimum of the loss surface (i.e. its parameter space or moduli space). Then, is there a way to find equivalent models of this trained model? 

\begin{definition}
\textit{Two models are equivalent} if they both converge on a fixed training set.
\end{definition}

It is necessary to ask this question so that the trained model can be understood better to a point that the distribution of its weights can be studied using random matrix theory, and if all the other equivalent models can be derived instantaneously, then this is also equivalent to apply quantum computing to find all those equivalent solutions in parallel. 

If a model can be replaced by another equivalent model, then the other model might be at the different point on the loss surface, then the barrier between the local minimum to the nearest lower local minimum might have a lower height or in some better case with a negative height, meaning the model can be trained to a better model with a smaller error by using a new set of training data to descent to that lower local minimum. Hence, it is also sufficient to ask the question: how to find equivalent models of a given trained model? If the answer to this question is unknown, suppose a trained model cannot distinguish the fake data generated by an adversary model from the true data, or if a model needs to go through a debugging process for some false classified input, then the model not only has to be retrained, but it cannot be justified it is the best solution due to the process to reach the solution is random. Thus, on the contrary, if all of its equivalent models can be found immediately, then chances are, within the set of equivalent models, there might exist a model that will not take the fake input or falsely classified a corner case, meaning a model can be replaced instantly without going through a new training process.


\section{Algorithm}
In Euclidean space, equivalent models of a trained model can be obtained by redefining matrix multiplication using Euclidean distance $d_E(x,y)=\vert x-y\vert$ for all $x, y\in\mathbb{R}$. For instance, let $A$ be in $\mathbb{R}^{n\times m}$, $B$ be in $\mathbb{R}^{d\times m}$, , and their entries are denoted by $a_{ki}$ and $b_{li}$ where $k\in\left\lbrace 1,...,n\right\rbrace$, $l\in\left\lbrace 1,...,d\right\rbrace$, and $i\in\left\lbrace 1,...,m\right\rbrace$. Then, $\left( A B^t\right)_{kl}=\sum\limits_{i=1}^m a_{ki}b_{il}$.

Instead of using matrix multiplication, a new operation using Euclidean distance is defined in the following:
$$
\left( A\odot_{E} B^t \right)_{kl}:= \sum\limits_{i=1}^m d_{E} \left( a_{ki} , b_{il} \right).
$$

Now, let the above $A$ and $B$ be any two matrices that need to be multiplied in a given model that already converges under the above new operation $\odot_{E}$ between matrices $A$ and $B$. Then by adding a real number $\alpha$ to $A$ and $B$, infinite many equivalent models can be derived, since
$$
\left( \left( \alpha + A \right) \odot_{E}\left( \alpha+ B^t \right)\right)_{kl} := \sum\limits_{i=1}^m d_{E} \left(\alpha + a_{ki} ,\alpha + b_{il} \right) = \sum\limits_{i=1}^m d_{E} \left( a_{ki} ,  b_{il} \right) = \left( A\odot_{E} B^t\right)_{kl}.
$$
In other words, the original given model is unchanged. This operation results in a translation of the loss surface. 

Alternatively, every entry of the matrices $A\in\mathbb{R}^{n\times m}$ and $B \in\mathbb{R}^{d\times m}$ can be embedded into the upper half-plane $\mathbb{H}^2$. That is, the entries of $A$ and $B$ become complex numbers with positive imaginary part: $a_{ki}\in\mathbb{H}^2$ and $b_{il}\in\mathbb{H}^2$.

Define a new binary operation between $A$ and $B$ using hyperbolic length on the upper-half plane $\mathbb{H}^2$\cite{beardon2012geometry}:
$$
\left( A\odot_{\mathbb{H}^2} B^t \right)_{kl} :=\sum\limits_{i=1}^m d_{\mathbb{H}^2} \left( a_{ki} , b_{il} \right).
$$

Then on the upper-half plane, the projective special linear group $\text{PSL}(2,\mathbb{R})$ is its automorphism group. Furthermore, elements of the group are conformal mapping that preserve not only angles but hyperbolic distance. Hence, $\text{PSL}(2,\mathbb{R})$ is also the isometry group of the upper-half plane.


To derive an equivalent model, take any $M\in \text{PSL}(2,\mathbb{R})\setminus\left\lbrace \text{id}\right\rbrace$, then there exists an isomorphism to map $M$ to a linear fractional (M\"{o}bius) map $T_M(z)=\frac{az+b}{cz+d}, a,b,c,d\in\mathbb{R},z\in\mathbb{C}$.
\begin{definition}[Hyperbolic-Orbit]
Define \textit{the hyperbolic-orbit product} is the following new operation: 
$$
\left( T_M(A) \odot_{\mathbb{H}^2} T_M(B^t)\right)_{kl} := \sum\limits_{i=1}^m d_{\mathbb{H}^2} \left(T_M \left( a_{ki} \right) ,T_M\left( b_{il}\right) \right).
$$
\end{definition}
\begin{definition}
Let $W$ be a neural network model and assume $W$ converged already with the hyperbolic-orbit product implemented. Then with the above notations, the orbit of $W$ of the given weights $\left\lbrace a_{ki},  b_{il}\right\rbrace$ (points in the hyperbolic space) is the set:
$$
\left\lbrace T_M(a_{ki}) \right\rbrace \cup \left\lbrace T_M(b_{il}) \right\rbrace
$$
for all $M\in \text{PSL}(2,\mathbb{R})$, and for all $k\in\left\lbrace 1,...,n\right\rbrace$, $l\in\left\lbrace 1,...,d\right\rbrace$, and $i\in\left\lbrace 1,...,m\right\rbrace$.
\end{definition}
\begin{proposition}
With the above notations, then the newly defined product is an invariant:
$$
\left( T_M(A) \odot_{\mathbb{H}^2} T_M(B^t)\right)_{kl} = \sum\limits_{i=1}^m d_{\mathbb{H}^2} \left(T_M \left( a_{ki} \right) ,T_M\left( b_{il}\right) \right)
=  \sum\limits_{i=1}^m d_{\mathbb{H}^2} \left( a_{ki}  , b_{il} \right)  =\left( A\odot_{\mathbb{H}^2} B^t\right)_{kl}.$$
\end{proposition}
\begin{proof}
This result follows immediately from the property of the M\"{o}bius mapping $T_M$ when it is corresponding to a matrix $M$ in the projective special linear group $\text{PSL}(2,\mathbb{R})$, then $T_M$ is an isometry under the hyperbolic metric.
\end{proof}
Let $\mathbb{B}^2:=\left\lbrace z\in\mathbb{C} : \vert z \vert <1\right\rbrace$ be the Poincare's disc and $\Psi$ be the Cayley's transformation $\Psi: \mathbb{H}^2 \to \mathbb{B}^2$. Then, $\Psi\left( T_M(z)\right)=\frac{a'z+b'}{c'z+d'}$ where $a',b',c',d'\in\mathbb{C},z\in\mathbb{C}$.

\begin{proposition}
With the above definition and proposition, then the value of the product has the same value when the points $a_{ki}$ and $b_{il}$ are sent to Poincare's disc:
$$
\left( T_M(A) \odot_{\mathbb{H}^2} T_M(B^t)\right)_{kl} = \sum\limits_{i=1}^m d_{\mathbb{H}^2} \left(T_M \left( a_{ki} \right) ,T_M\left( b_{il}\right) \right)
=  \sum\limits_{i=1}^m \rho_{\mathbb{B}^2} \left(\Psi \left(T_M \left( a_{ki} \right)\right) ,\Psi\left( T_M\left( b_{il}\right) \right)\right) .$$
\end{proposition}
\begin{proof}
Since for each $z_1,z_2\in\mathbb{H}^2$, $d_{\mathbb{H}^2}(z_1,z_2)=\rho_{\mathbb{B}^2}(\Psi(z_1),\Psi(z_2)).$
\end{proof}

\section{Example}
All layers of a given neural network model can be implemented with the above hyperbolic-orbit algorithm. The following is an example when the hyperbolic-orbit algorithm is applied to the self-attention mechanism, i.e. transformer-based models.
Let $A=Q\in \mathbb{C}^{n\times d_q}$ and $B=K^t\in \mathbb{C}^{d_k\times n}$ in self-attention mechanism\cite{vaswani2017attention} using dynamic scaling factor $\beta$ (defined in our another work in progress), and by convention let $d_q=d_k$.

\section{Discussion}
With the hyperbolic-orbit algorithm, it is possible to find an appropriate M\"{o}bius mapping that is corresponding to $M\in\text{PSL}(2,\mathbb{R})$ such that training a 3-node neural network become not necessarily NP-hard in \cite{blum1988training}. Further, if every layer of a given neural network is implemented with the hyperbolic-orbit algorithm introduced in this paper, by combining with perturbations and parallel computing, the steepest descent path toward the global minimum could be designed. The algorithm introduced by the paper can help to save training time for parallel computing in the ensemble method, that is, instead of starting over from the beginning to train $N'$ models in parallel, $N'\in\mathbb{N}$, the training can start with $N'$ models that are equivalent to a trained model that already has an error that cannot be reduced by using the old training set--but on the orbits of the trained model, each equivalent model on the orbit can be kept training with a new training set by updating the weight matrices using back-propagation with the gradient descent algorithm. Then, in each epoch, the best model could be selected and instantaneously its $N'$ equivalent models could be obtained using the hyperbolic-orbit algorithm.


%\section{Limitation}



%\begin{lstlisting}[language=iPython]
%<- #there shouldn't be quotation marks
%"""
%---------
%sin2_theta  = np.sin(theta)**2 - a + b
%"""
%import math
%import numpy as np
%from lib.analytical import csa
%MAS = math.degrees(math.acos(math.sqrt(1/3)))/360 * 2* math.pi + a -b
%\end{lstlisting}

\bibliographystyle{amsplain} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file
\end{document}


