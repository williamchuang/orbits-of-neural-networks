\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ma2018mode}
\citation{tramer2017ensemble}
\citation{athalye2018obfuscated}
\citation{yun2017global}
\citation{blum1988training}
\citation{judd1990neural}
\citation{li2018visualizing}
\citation{garipov2018loss}
\citation{malan2021survey}
\citation{draxler2018essentially}
\citation{freeman2016topology}
\citation{du2019gradient}
\citation{goodfellow2014qualitatively}
\citation{im2016empirical}
\citation{dinh2017sharp}
\citation{kawaguchi2017generalization}
\citation{neyshabur2017exploring}
\citation{nguyen2018loss}
\citation{choromanska2015loss}
\citation{nguyen2017loss}
\citation{kumar2023black}
\citation{poole2016exponential}
\citation{zhang2021embedding}
\citation{lucas2022optimization}
\citation{bronstein2021geometric}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{tu2011manifolds}
\citation{louart2018random}
\citation{pennington2017geometry}
\citation{seddik2020random}
\citation{li2018visualizing}
\citation{garipov2018loss}
\citation{malan2021survey}
\citation{draxler2018essentially}
\citation{freeman2016topology}
\citation{du2019gradient}
\citation{bronstein2021geometric}
\citation{bronstein2021geometric}
\@writefile{toc}{\contentsline {section}{\numberline {2}Hyperbolic-length Product}{4}{section.2}\protected@file@percent }
\citation{beardon2012geometry}
\citation{MR1138441}
\citation{MR725161}
\citation{beardon2012geometry}
\citation{MR2402415}
\citation{MR4221225}
\citation{MR1893917}
\@writefile{toc}{\contentsline {section}{\numberline {3}Hyperbolic-orbit Algorithm}{7}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The computational graph of case 1.}}{9}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An illustration on how to embed \textbf  {any} given product of matrices $AB$ in \textbf  {any neural network models} to hyperbolic space $\mathbb  {H}^2$.}}{10}{figure.2}\protected@file@percent }
\citation{vaswani2017attention}
\citation{chuang2022hausdorff}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The computational graph of Case 2 when a hyperbolic-length product is implemented to the model directly before training.}}{11}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Examples}{11}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Self-attention mechanism in any transformer-based models with the hyperbolic-length product implemented before training}{11}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Empirical Results}{11}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{12}{section.6}\protected@file@percent }
\citation{blum1988training}
\citation{seppala2011geometry}
\citation{bers1981finite}
\citation{cheng1980existence}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Using an open set $B_r(p)$ centered at $p\in \mathbb  {H}^2$ with radius $r>0$ that covers all embedded weights of the given converged model to study the correspondence between the location of $B_r(p)$ in $\mathbb  {H}^2$ and the location of the model in the loss surface.}}{14}{figure.4}\protected@file@percent }
\citation{thurston2022geometry}
\citation{seppala2011geometry}
\citation{gromov1981hyperbolic}
\citation{thurston1982three}
\citation{mcmullen1992riemann}
\citation{benedetti20072+}
\citation{thurston2022geometry}
\citation{seppala2011geometry}
\citation{gromov1981hyperbolic}
\citation{thurston1982three}
\citation{mcmullen1992riemann}
\citation{benedetti20072+}
\citation{chuang2015implications}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Focusing on a two-generator subgroup of $\text  {PSL}(2,\mathbb  {R})$ (it can be called a Schottky group, Fuchsian group, or Kleinian group) and its fundamental domain in $\mathbb  {H}^2$. \textbf  {The geometry, topology, and the group generators of the hyperbolic manifold\cite  {thurston2022geometry, seppala2011geometry, gromov1981hyperbolic, thurston1982three, mcmullen1992riemann, benedetti20072+} determine the gradients that are provided by the hyperbolic-orbit algorithm.}}}{16}{figure.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Different configurations can provide different oriented Riemannian manifold.}}{17}{figure.6}\protected@file@percent }
\citation{seppala2011geometry}
\citation{sullivan1979hyperbolic}
\citation{li2018visualizing}
\citation{garipov2018loss}
\citation{malan2021survey}
\citation{draxler2018essentially}
\citation{freeman2016topology}
\citation{du2019gradient}
\citation{seppala2011geometry}
\citation{ford2004automorphic}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Possible scenarios of loss surface. Each possible scenario of the new loss surface after applying fractional linear mapping (which is corresponding to an element to the two-generator group) to the weights of the given converged model. Notice that it is possible that the model was mapped to the deleted neighborhood of a new local minimum or the global minimum.}}{18}{figure.7}\protected@file@percent }
\citation{borthwick2007spectral}
\citation{zhang2022toward}
\bibstyle{amsplain}
\bibdata{refs}
\bibcite{MR725161}{1}
\bibcite{athalye2018obfuscated}{2}
\bibcite{beardon2012geometry}{3}
\bibcite{benedetti20072+}{4}
\bibcite{bers1981finite}{5}
\bibcite{blum1988training}{6}
\bibcite{borthwick2007spectral}{7}
\bibcite{bronstein2021geometric}{8}
\bibcite{cheng1980existence}{9}
\bibcite{choromanska2015loss}{10}
\bibcite{chuang2022hausdorff}{11}
\bibcite{chuang2015implications}{12}
\bibcite{dinh2017sharp}{13}
\bibcite{draxler2018essentially}{14}
\bibcite{du2019gradient}{15}
\bibcite{ford2004automorphic}{16}
\bibcite{freeman2016topology}{17}
\bibcite{garipov2018loss}{18}
\bibcite{goodfellow2014qualitatively}{19}
\bibcite{gromov1981hyperbolic}{20}
\bibcite{im2016empirical}{21}
\bibcite{judd1990neural}{22}
\bibcite{MR1138441}{23}
\bibcite{MR2402415}{24}
\bibcite{kawaguchi2017generalization}{25}
\bibcite{kumar2023black}{26}
\bibcite{li2018visualizing}{27}
\bibcite{louart2018random}{28}
\bibcite{lucas2022optimization}{29}
\bibcite{ma2018mode}{30}
\bibcite{malan2021survey}{31}
\bibcite{mcmullen1992riemann}{32}
\bibcite{neyshabur2017exploring}{33}
\bibcite{nguyen2017loss}{34}
\bibcite{nguyen2018loss}{35}
\bibcite{pennington2017geometry}{36}
\bibcite{poole2016exponential}{37}
\bibcite{MR4221225}{38}
\bibcite{MR1893917}{39}
\bibcite{seddik2020random}{40}
\bibcite{seppala2011geometry}{41}
\bibcite{sullivan1979hyperbolic}{42}
\bibcite{thurston1982three}{43}
\bibcite{thurston2022geometry}{44}
\bibcite{tramer2017ensemble}{45}
\bibcite{tu2011manifolds}{46}
\bibcite{vaswani2017attention}{47}
\bibcite{yun2017global}{48}
\bibcite{zhang2021embedding}{49}
\bibcite{zhang2022toward}{50}
\gdef \@abspage@last{24}
